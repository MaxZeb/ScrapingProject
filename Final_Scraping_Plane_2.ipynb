{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "The scikit package for python will be one of the most important ones during the master program. Therefore I came up with the idea to get a overview what topics are coverd on the documentation webpage and especially which external resources are provided. The apporoch would be to start at the highest level of the webpage and then go down the tree shaped website directory. I limit the analysis to 2 levels below the starting point, otherwise it would be computational challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(DRs):\n",
    "    try:\n",
    "        if len(DRs.get(\"class\")) > 1: \n",
    "            class_lap = str(\"\")\n",
    "            for i in range(len(DRs.get(\"class\"))):\n",
    "                class_lap = str(class_lap + DRs.get(\"class\")[i])\n",
    "            return class_lap\n",
    "        else: \n",
    "            return DRs.get(\"class\")\n",
    "    except:\n",
    "        \"Not Labeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def website_type(webdir):\n",
    "    try: \n",
    "        if (webdir[0:4] == \"http\") or (webdir[-3:] == \"pdf\") or (webdir[0:2] == \"..\"):\n",
    "            return \"foreign\"\n",
    "        else: \n",
    "            return \"scikit\"\n",
    "    except: \n",
    "        return \"foreign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_scikit(webdir):\n",
    "    if website_type(webdir) == \"scikit\":\n",
    "        URL = \"http://scikit-learn.org/stable/\"\n",
    "        # webdir = \"documentation.html\"\n",
    "        next_url = str(URL + webdir)\n",
    "        soup = BeautifulSoup(requests.get(next_url).content, \"lxml\")\n",
    "        link_data = pd.DataFrame([])\n",
    "        if len(soup.select(\"div.body a\")) > 0:\n",
    "            for DRs in soup.select(\"div.body a\"):\n",
    "                link_data = link_data.append(pd.DataFrame({\"Website\": URL, \"Location\": webdir, \n",
    "                                                           \"Links\": DRs.get(\"href\"), \"Type\": get_class(DRs)}, \n",
    "                                                          index=[0])\n",
    "                                             , ignore_index=True)\n",
    "        else: \n",
    "            link_data = pd.DataFrame({\"Website\": URL, \"Location\": webdir, \"Links\": \"No Link here\", \"Type\": \"Not Labeled\"}, \n",
    "                                                      index=[0])\n",
    "    else: \n",
    "        link_data = pd.DataFrame({\"Website\": webdir, \"Location\": webdir, \"Links\": \"No Link here\", \"Type\": \"Not Labeled\"}, \n",
    "                                                      index=[0])\n",
    "    return link_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_worker(i):\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        if item == 'break':\n",
    "            break\n",
    "        results = get_links_scikit(item)\n",
    "        q.task_done()\n",
    "        if (q.unfinished_tasks in [2000,1500,1000,500,10]) or (q.unfinished_tasks < 10):\n",
    "            print(\"Task_done & amount of unfinished sub tasks: \" + str(q.unfinished_tasks))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_worker(i):\n",
    "    while True:\n",
    "        item = m.get()\n",
    "        if item == 'break':\n",
    "            break\n",
    "        results = get_links_scikit(item)\n",
    "        if any(results):\n",
    "            for x in results[\"Links\"]:\n",
    "                q.put(x)\n",
    "        m.task_done()\n",
    "        print(\"Task_done & amount of unfinished tasks: \" + str(q.unfinished_tasks))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving information from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task_done & amount of unfinished tasks: 1\n",
      "Task_done & amount of unfinished tasks: 2\n",
      "Task_done & amount of unfinished tasks: 49\n",
      "Task_done & amount of unfinished tasks: 80\n",
      "Task_done & amount of unfinished tasks: 121\n",
      "Task_done & amount of unfinished tasks: 209\n",
      "Task_done & amount of unfinished tasks: 773\n",
      "Task_done & amount of unfinished tasks: 801\n",
      "Task_done & amount of unfinished tasks: 851\n",
      "Task_done & amount of unfinished tasks: 852\n",
      "Task_done & amount of unfinished tasks: 1331\n",
      "Task_done & amount of unfinished tasks: 2004\n",
      "Task_done & amount of unfinished tasks: 2080\n"
     ]
    }
   ],
   "source": [
    "m = queue.Queue()\n",
    "q = queue.Queue()\n",
    "initial_tasks = list(get_links_scikit(\"documentation.html\")[\"Links\"])\n",
    "for y in initial_tasks:\n",
    "    m.put(y)\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    m_results = executor.map(hard_worker,range(m.unfinished_tasks))\n",
    "    m.join()\n",
    "    for i in range(4):\n",
    "        m.put('break')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task_done & amount of unfinished sub tasks: 2000\n",
      "Task_done & amount of unfinished sub tasks: 1500\n",
      "Task_done & amount of unfinished sub tasks: 1000\n",
      "Task_done & amount of unfinished sub tasks: 500\n",
      "Task_done & amount of unfinished sub tasks: 10\n",
      "Task_done & amount of unfinished sub tasks: 9\n",
      "Task_done & amount of unfinished sub tasks: 8\n",
      "Task_done & amount of unfinished sub tasks: 7\n",
      "Task_done & amount of unfinished sub tasks: 6\n",
      "Task_done & amount of unfinished sub tasks: 5\n",
      "Task_done & amount of unfinished sub tasks: 4\n",
      "Task_done & amount of unfinished sub tasks: 3\n",
      "Task_done & amount of unfinished sub tasks: 2\n",
      "Task_done & amount of unfinished sub tasks: 1\n",
      "Task_done & amount of unfinished sub tasks: 0\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "    futures = executor.map(simple_worker,range(q.unfinished_tasks))\n",
    "    q.join()\n",
    "    for i in range(25):\n",
    "        q.put('break')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links_full = pd.DataFrame([])\n",
    "for value in m_results:\n",
    "    links_full = links_full.append(value,ignore_index=True)\n",
    "for value in futures:\n",
    "    links_full = links_full.append(value,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_full[\"url\"] = links_full.Links.apply(lambda x: urlparse(x).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997     https://www.youtube.com/watch?v=Zd5dfooZWG4\n",
       "2001     https://www.youtube.com/watch?v=cHZONQ2-x7I\n",
       "11651    https://www.youtube.com/watch?v=Jm-eBD9xR3w\n",
       "Name: Links, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all youtube links from scikit\n",
    "links_full[links_full['url'].str.contains('youtube', na = False)].Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_full.to_csv(\"scikit_FullLinks.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
